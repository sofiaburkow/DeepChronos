/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

Preparing datasets (resampled)...

Preparing multi_step train' dataset...
Original label distribution (train): Counter({np.int64(0): 55178, np.int64(1): 20252, np.int64(2): 20252, np.int64(3): 20252, np.int64(4): 20252, np.int64(5): 20252})
Loading cached train dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_multi_step_resampled.pkl
Label distribution: Counter({'no_alarm': 136186, 'alarm': 20252})

Preparing multi_step test' dataset...
Original label distribution (test): Counter({np.int64(0): 36785, np.int64(5): 13502, np.int64(3): 14, np.int64(2): 9, np.int64(4): 9, np.int64(1): 8})
Loading cached test dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_multi_step_resampled.pkl
Label distribution: Counter({'no_alarm': 36825, 'alarm': 13502})

Using pretrained models: False

Caching ACs
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:8.3512 	Average Loss:  0.6064330407232046
Iteration:  200 	s:8.0891 	Average Loss:  0.47025866507552566
Iteration:  300 	s:8.1167 	Average Loss:  0.331321472406853
Iteration:  400 	s:8.1179 	Average Loss:  0.30559170106920647
Iteration:  500 	s:8.0423 	Average Loss:  0.2769984984092298
Iteration:  600 	s:8.1095 	Average Loss:  0.2786928127081046
Iteration:  700 	s:8.1028 	Average Loss:  0.2577366093526507
Iteration:  800 	s:8.0907 	Average Loss:  0.2562391099650631
Iteration:  900 	s:8.0598 	Average Loss:  0.23817552687993157
Iteration:  1000 	s:8.0394 	Average Loss:  0.24078328546605918
Iteration:  1100 	s:8.1289 	Average Loss:  0.2370496202571121
Iteration:  1200 	s:8.0942 	Average Loss:  0.2202037893024044
Iteration:  1300 	s:8.1708 	Average Loss:  0.22303309987651118
Iteration:  1400 	s:8.1888 	Average Loss:  0.2191219350954225
Iteration:  1500 	s:8.0849 	Average Loss:  0.2092691709504743
Iteration:  1600 	s:8.1411 	Average Loss:  0.2090630666754032
Iteration:  1700 	s:8.1586 	Average Loss:  0.20330879386511697
Iteration:  1800 	s:8.2415 	Average Loss:  0.21372624389607153
Iteration:  1900 	s:8.1430 	Average Loss:  0.20317270782405786
Iteration:  2000 	s:8.1154 	Average Loss:  0.20993268200399826
Iteration:  2100 	s:8.1585 	Average Loss:  0.20902895718927084
Iteration:  2200 	s:8.1323 	Average Loss:  0.20414674001484173
Iteration:  2300 	s:8.1176 	Average Loss:  0.1967910213796904
Iteration:  2400 	s:8.0873 	Average Loss:  0.19900321207960814
Iteration:  2500 	s:8.1069 	Average Loss:  0.19729906841106526
Iteration:  2600 	s:8.0983 	Average Loss:  0.1962846637899304
Iteration:  2700 	s:8.1058 	Average Loss:  0.1959170315393817
Iteration:  2800 	s:8.0754 	Average Loss:  0.19614470276691123
Iteration:  2900 	s:10.7142 	Average Loss:  0.19930277438237226
Iteration:  3000 	s:8.2735 	Average Loss:  0.19667256838086986
Iteration:  3100 	s:8.1815 	Average Loss:  0.19858532184834246
Epoch time:  257.17310881614685
Accuracy:  0.7300057623144635
