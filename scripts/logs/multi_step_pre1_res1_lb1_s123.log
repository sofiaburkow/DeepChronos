/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

Preparing datasets (resampled)...

Preparing multi_step train' dataset...
Original label distribution (train): Counter({np.int64(0): 55178, np.int64(1): 20252, np.int64(2): 20252, np.int64(3): 20252, np.int64(4): 20252, np.int64(5): 20252})
Loading cached train dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_multi_step_resampled.pkl
Label distribution: Counter({'no_alarm': 136186, 'alarm': 20252})

Preparing multi_step test' dataset...
Original label distribution (test): Counter({np.int64(0): 36785, np.int64(5): 13502, np.int64(3): 14, np.int64(2): 9, np.int64(4): 9, np.int64(1): 8})
Loading cached test dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_multi_step_resampled.pkl
Label distribution: Counter({'no_alarm': 36825, 'alarm': 13502})

Using pretrained models: True
Loading pretrained model for phase 1...
Loading pretrained model for phase 2...
Loading pretrained model for phase 3...
Loading pretrained model for phase 4...
Loading pretrained model for phase 5...

Caching ACs
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:8.6054 	Average Loss:  0.24496327337824503
Iteration:  200 	s:8.4226 	Average Loss:  0.21843243697348683
Iteration:  300 	s:8.1562 	Average Loss:  0.2078578594202739
Iteration:  400 	s:8.0606 	Average Loss:  0.20818989337463886
Iteration:  500 	s:8.0296 	Average Loss:  0.21204716302468024
Iteration:  600 	s:8.0408 	Average Loss:  0.20120329211017343
Iteration:  700 	s:8.2303 	Average Loss:  0.19978472556748067
Iteration:  800 	s:8.1962 	Average Loss:  0.197963384687886
Iteration:  900 	s:8.2300 	Average Loss:  0.19126017770069087
Iteration:  1000 	s:8.3388 	Average Loss:  0.19431155677122963
Iteration:  1100 	s:8.2752 	Average Loss:  0.20271592726754842
Iteration:  1200 	s:8.1929 	Average Loss:  0.19772021199711975
Iteration:  1300 	s:8.1774 	Average Loss:  0.19660124366639034
Iteration:  1400 	s:8.2156 	Average Loss:  0.19384935930548364
Iteration:  1500 	s:8.1297 	Average Loss:  0.1985319531818569
Iteration:  1600 	s:8.1767 	Average Loss:  0.19279759208276823
Iteration:  1700 	s:8.2556 	Average Loss:  0.1955994937570382
Iteration:  1800 	s:8.1276 	Average Loss:  0.19434546455947369
Iteration:  1900 	s:8.2266 	Average Loss:  0.19640556850010576
Iteration:  2000 	s:8.1187 	Average Loss:  0.19614161833887167
Iteration:  2100 	s:8.1317 	Average Loss:  0.19741313615050363
Iteration:  2200 	s:8.1330 	Average Loss:  0.19529501413591427
Iteration:  2300 	s:8.1381 	Average Loss:  0.19994645721553028
Iteration:  2400 	s:8.2051 	Average Loss:  0.19165895894901955
Iteration:  2500 	s:8.1948 	Average Loss:  0.19257878393218975
Iteration:  2600 	s:8.1115 	Average Loss:  0.1909179298901921
Iteration:  2700 	s:8.1941 	Average Loss:  0.19314062566136886
Iteration:  2800 	s:8.1731 	Average Loss:  0.19067816839684568
Iteration:  2900 	s:8.2146 	Average Loss:  0.19528233181542567
Iteration:  3000 	s:8.1732 	Average Loss:  0.18952377199203208
Iteration:  3100 	s:8.1820 	Average Loss:  0.19681678916202397
Epoch time:  256.57772040367126
Accuracy:  0.7317344566534862
