/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

=== Running experiment: multi_step_from_scratch_resampled_lookback20000 ===

--- Preparing multi_step train dataset ---
Using resampled data
Using lookback window of size 20000 for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_resampled_lookback20000.pkl
Label distribution: Counter({'no_alarm': 151922, 'alarm': 4516})

--- Preparing multi_step test dataset ---
Using resampled data
Using lookback window of size 20000 for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_resampled_lookback20000.pkl
Label distribution: Counter({'no_alarm': 43930, 'alarm': 6397})

--- Initializing networks and building DeepProbLog model ---
Using pretrained models: False
Caching ACs

--- Training multi_step DeepProbLog model with batch size 50 ---
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:6.4878 	Average Loss:  13.317943910506562
Iteration:  200 	s:5.0027 	Average Loss:  13.580083413275334
Iteration:  300 	s:5.6731 	Average Loss:  13.725799885934794
Iteration:  400 	s:4.4182 	Average Loss:  13.759069498423699
Iteration:  500 	s:4.6641 	Average Loss:  13.582840093274097
Iteration:  600 	s:4.4452 	Average Loss:  13.600005408888574
Iteration:  700 	s:4.3966 	Average Loss:  13.485095718251868
Iteration:  800 	s:6.9436 	Average Loss:  14.119357267994582
Iteration:  900 	s:6.4044 	Average Loss:  13.589607875840075
Iteration:  1000 	s:6.1978 	Average Loss:  13.32012058801857
Iteration:  1100 	s:5.6407 	Average Loss:  13.380991546123404
Iteration:  1200 	s:5.8120 	Average Loss:  13.747222192842106
Iteration:  1300 	s:8.1102 	Average Loss:  13.220978131342317
Iteration:  1400 	s:20.2534 	Average Loss:  13.58057747789485
Iteration:  1500 	s:21.0589 	Average Loss:  13.420700974193412
Iteration:  1600 	s:21.0375 	Average Loss:  13.55626507337103
Iteration:  1700 	s:20.6401 	Average Loss:  13.542551033964548
Iteration:  1800 	s:17.5121 	Average Loss:  13.451683426720388
Iteration:  1900 	s:6.1527 	Average Loss:  13.881423148679847
Iteration:  2000 	s:10.8471 	Average Loss:  13.678915565740104
Iteration:  2100 	s:7.6334 	Average Loss:  13.739358870420787
Iteration:  2200 	s:5.0258 	Average Loss:  13.417013637704331
Iteration:  2300 	s:4.9738 	Average Loss:  13.477650758948212
Iteration:  2400 	s:5.4189 	Average Loss:  13.561186017528009
Iteration:  2500 	s:5.2781 	Average Loss:  13.306067702457792
Iteration:  2600 	s:4.6208 	Average Loss:  13.410791671164509
Iteration:  2700 	s:5.2239 	Average Loss:  13.597373435256365
Iteration:  2800 	s:5.7707 	Average Loss:  13.43193798718295
Iteration:  2900 	s:4.6557 	Average Loss:  13.823530833553383
Iteration:  3000 	s:5.2458 	Average Loss:  13.574941071521135
Iteration:  3100 	s:6.4057 	Average Loss:  13.959777752108877
Epoch time:  253.8846914768219
