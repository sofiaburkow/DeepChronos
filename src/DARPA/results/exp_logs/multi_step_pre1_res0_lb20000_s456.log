/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

=== Running experiment: multi_step_pretrained_original_lookback20000 ===

--- Preparing multi_step train dataset ---
Using original data
Using lookback window of size 20000 for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_original_lookback20000.pkl
Label distribution: Counter({'no_alarm': 70973, 'alarm': 4516})

--- Preparing multi_step test dataset ---
Using original data
Using lookback window of size 20000 for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_original_lookback20000.pkl
Label distribution: Counter({'no_alarm': 43930, 'alarm': 6397})

--- Initializing networks and building DeepProbLog model ---
Using pretrained models: True
Loading pretrained model for phase 1...
Loading pretrained model for phase 2...
Loading pretrained model for phase 3...
Loading pretrained model for phase 4...
Loading pretrained model for phase 5...
Caching ACs

--- Training multi_step DeepProbLog model with batch size 50 ---
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:6.7354 	Average Loss:  5.773675181042823
Iteration:  200 	s:6.7607 	Average Loss:  5.732645222339309
Iteration:  300 	s:6.5284 	Average Loss:  6.143630417751876
Iteration:  400 	s:8.6339 	Average Loss:  5.855505818591722
Iteration:  500 	s:7.7569 	Average Loss:  5.556220138333377
Iteration:  600 	s:8.0907 	Average Loss:  6.019646567470328
Iteration:  700 	s:8.0420 	Average Loss:  5.861976629835156
Iteration:  800 	s:7.3398 	Average Loss:  6.013785944811262
Iteration:  900 	s:6.5981 	Average Loss:  5.902892071676534
Iteration:  1000 	s:6.6278 	Average Loss:  5.899668473832801
Iteration:  1100 	s:6.7038 	Average Loss:  5.8964659768567405
Iteration:  1200 	s:7.9722 	Average Loss:  5.818405813381632
Iteration:  1300 	s:7.9657 	Average Loss:  5.74195113467687
Iteration:  1400 	s:7.8215 	Average Loss:  5.767960317531411
Iteration:  1500 	s:7.5258 	Average Loss:  5.8224165656743745
Epoch time:  112.19227957725525
