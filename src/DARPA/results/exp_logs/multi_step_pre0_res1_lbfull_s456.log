/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

=== Running experiment: multi_step_from_scratch_resampled_full_lookback ===

--- Preparing multi_step train dataset ---
Using resampled data
Using full history for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_resampled_full_lookback.pkl
Label distribution: Counter({'no_alarm': 136186, 'alarm': 20252})

--- Preparing multi_step test dataset ---
Using resampled data
Using full history for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_resampled_full_lookback.pkl
Label distribution: Counter({'no_alarm': 36825, 'alarm': 13502})

--- Initializing networks and building DeepProbLog model ---
Using pretrained models: False
Caching ACs

--- Training multi_step DeepProbLog model with batch size 50 ---
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:8.6444 	Average Loss:  0.6401443401165307
Iteration:  200 	s:8.3095 	Average Loss:  0.5244167502585333
Iteration:  300 	s:9.6658 	Average Loss:  0.39775563415780196
Iteration:  400 	s:10.0338 	Average Loss:  0.32434426888474266
Iteration:  500 	s:24.0298 	Average Loss:  0.2961878257944772
Iteration:  600 	s:25.5808 	Average Loss:  0.2762988806199428
Iteration:  700 	s:10.7340 	Average Loss:  0.25631905892387297
Iteration:  800 	s:8.3409 	Average Loss:  0.2581764487420514
Iteration:  900 	s:8.2503 	Average Loss:  0.23907763758115835
Iteration:  1000 	s:8.5998 	Average Loss:  0.24297994751241275
Iteration:  1100 	s:11.4069 	Average Loss:  0.23656017949821945
Iteration:  1200 	s:9.2691 	Average Loss:  0.22427840929039122
Iteration:  1300 	s:10.6896 	Average Loss:  0.2150179495290149
Iteration:  1400 	s:10.7745 	Average Loss:  0.2217857681886744
Iteration:  1500 	s:10.0700 	Average Loss:  0.21673810721214068
Iteration:  1600 	s:9.1315 	Average Loss:  0.22139332802404396
Iteration:  1700 	s:8.9004 	Average Loss:  0.2119766420001258
Iteration:  1800 	s:7.9658 	Average Loss:  0.21344558961940607
Iteration:  1900 	s:7.8612 	Average Loss:  0.2132350320870023
Iteration:  2000 	s:10.0504 	Average Loss:  0.1993749586394256
Iteration:  2100 	s:9.7227 	Average Loss:  0.1982919510216709
Iteration:  2200 	s:9.7678 	Average Loss:  0.19994168195204054
Iteration:  2300 	s:7.9387 	Average Loss:  0.20600036408355152
Iteration:  2400 	s:7.8465 	Average Loss:  0.20296036813991036
Iteration:  2500 	s:7.9588 	Average Loss:  0.20341282609861083
Iteration:  2600 	s:7.8061 	Average Loss:  0.2075027234804648
Iteration:  2700 	s:7.8267 	Average Loss:  0.20282921807711632
Iteration:  2800 	s:7.9572 	Average Loss:  0.20174360268540112
Iteration:  2900 	s:8.3789 	Average Loss:  0.19163029581668836
Iteration:  3000 	s:8.2580 	Average Loss:  0.20618800146114538
Iteration:  3100 	s:8.3103 	Average Loss:  0.20284622841102404
Epoch time:  312.5324878692627
