/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/deepproblog/semiring/graph_semiring.py:77: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)
  return -self.eps <= float(a) <= self.eps

=== Running experiment: multi_step_pretrained_original_full_lookback ===

--- Preparing multi_step train dataset ---
Using original data
Using full history for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/train_original_full_lookback.pkl
Label distribution: Counter({'no_alarm': 55237, 'alarm': 20252})

--- Preparing multi_step test dataset ---
Using original data
Using full history for dataset preparation
Loading cached dataset from /home/sofia/Desktop/Thesis/DeepChronos/src/DARPA/data/cache/test_original_full_lookback.pkl
Label distribution: Counter({'no_alarm': 36825, 'alarm': 13502})

--- Initializing networks and building DeepProbLog model ---
Using pretrained models: True
Loading pretrained model for phase 1...
Loading pretrained model for phase 2...
Loading pretrained model for phase 3...
Loading pretrained model for phase 4...
Loading pretrained model for phase 5...
Caching ACs

--- Training multi_step DeepProbLog model with batch size 50 ---
Training  for 1 epoch(s)
Epoch 1
Iteration:  100 	s:8.0666 	Average Loss:  0.4679837494163822
Iteration:  200 	s:10.9263 	Average Loss:  0.4553835878608521
Iteration:  300 	s:10.8157 	Average Loss:  0.43631615535650636
Iteration:  400 	s:8.2062 	Average Loss:  0.42579708518645765
Iteration:  500 	s:10.0801 	Average Loss:  0.41804497886919173
Iteration:  600 	s:9.4354 	Average Loss:  0.4052803545826789
Iteration:  700 	s:10.2682 	Average Loss:  0.4066842105948346
Iteration:  800 	s:8.9631 	Average Loss:  0.40126071642463496
Iteration:  900 	s:9.4564 	Average Loss:  0.4015902561003978
Iteration:  1000 	s:9.9693 	Average Loss:  0.40593054795045896
Iteration:  1100 	s:10.5658 	Average Loss:  0.3957394909978653
Iteration:  1200 	s:11.8506 	Average Loss:  0.405902072532788
Iteration:  1300 	s:22.4179 	Average Loss:  0.4067740767409616
Iteration:  1400 	s:9.4516 	Average Loss:  0.4096956646423351
Iteration:  1500 	s:17.2717 	Average Loss:  0.39691346054536586
Epoch time:  169.59982419013977
