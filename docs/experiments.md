# ML Experiments

The goal of these experiments is to identify the most suitable model, feature set, and dataset split for flow-based intrusion detection on the DARPA 2000 dataset. 

## Experimental Setup

There are four variables that we want to experiment with:
- Different models
    - Classical ML models: SVM, Decision Tree, Random Forest
    - Deep Learning models: MLP, LSTM
- Different feature sets (based on flow features extracted from zeek conn logs)
    - Note: the goal is to remove features that might cause bias, and are not explainatory 
- Datasets
    - Which DARPA 2000 dataset to use:
        - Scenario one or two
        - Inside, outside (dmz), mixed
    - Split size (train/test/val)
    - Split method (stratified, temporal, host-temporal)
- Data balancing methods
    - No balancing
    - Undersampling
    - Oversampling
    - Class weighting

## Features

The following features are extracted from Zeek conn logs:
```bash
[
    "flow_id",
    "start_time",
    "end_time",
    "duration",
    "src_ip",
    "sport",
    "dst_ip",
    "dport",
    "proto",
    "service",
    "orig_bytes",
    "resp_bytes",
    "conn_state",
    "local_orig",
    "local_resp",
    "missed_bytes",
    "history",
    "orig_pkts",
    "orig_ip_bytes",
    "resp_pkts",
    "resp_ip_bytes",
    "tunnel_parents",
    "ip_proto" 
]
```
From these, we will experiment with different subsets to identify the most relevant features for intrusion detection.

Features that cause bias (and hinder generalization) will be removed, such as `src_ip`, `dst_ip`, `start_time`, and `end_time`.

## Data Preprocessing

Data preprocessing steps include:
- Filtering out irrelevant or redundant features
- Encoding categorical variables
- Normalizing numerical features
- Splitting the dataset into train/test sets based on the chosen method (stratified, temporal, host-temporal)
- Applying data balancing techniques if necessary
- Saving the preprocessed datasets for reproducibility
- Generating preprocessing reports and visualizations

### How to run preprocessing

From the root directory, execute the main preprocessing script with the desired parameters. For example:

```bash
uv run python experiments/preprocessing/preprocess_all.py 
```

Which preprocessing jobs to run are defined in the `experiments/preprocessing/all_jobs.json` file. You can modify this file to add or change preprocessing configurations.

There are also several command-line arguments available to customize the preprocessing, such as selecting feature sets, dataset splits, and balancing methods.

The processed datasets will be saved in the processed_data/ directory, organized by feature set and preprocessing parameters.

There are also Jupyter notebooks available in the `experiments/preprocessing/notebooks/` directory for interactive data exploration and preprocessing. The datasets generated by these notebooks are saved in the `experiments/preprocessing/processed_data_notebooks/` directory.

## Running Experiments

Once the data is preprocessed, you can run the experiments using the main experiment script. For example:

```bash
uv run python experiments/train_all.py --models-file experiments/models_temp.json --datasets-file experiments/datasets_temp.json --class-weights True
```

There are several command-line arguments available to customize the experiments, such as selecting models, datasets, and enabling class weights.

The results of the experiments will be saved in the `experiments/results/` directory, organized by model and dataset configuration. Informative logs will be generated in the `experiments/logs/` directory to track the progress and performance of each experiment.

## Results Analysis

Conclusion (for now):
- **One classifer per phase**
- Feature set: 
    - **Remove biased features**, then use all remaining features
- Dataset: scenario one:
    - **Inside only**
    - **Temporal split**
    - **Train size: 60/40** (should be experimented with more in the future)
- **LSTM** is best model (as it can capture temporal dependencies)
    - **Window size: 10**
- Balancing:
    - Class weighting or oversampling (not both)
    - For now: **class weighting**