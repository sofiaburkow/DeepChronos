COMMAND: uv run python experiments/models/lstm.py experiments/preprocessing/processed_data/temporal/unbalanced/w5/phase_1/ True
START: 20251220T103707Z

2025-12-20 11:37:08.670523: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:37:08.733884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-20 11:37:10.142397: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:37:11.327440: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Epoch 1/50
1062/1062 - 6s - 5ms/step - accuracy: 0.9955 - loss: 1.2892 - val_accuracy: 0.9691 - val_loss: 0.0500
Epoch 2/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9686 - loss: 0.1295 - val_accuracy: 0.9999 - val_loss: 0.0128
Epoch 3/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9991 - loss: 0.0203 - val_accuracy: 0.9997 - val_loss: 0.0037
Epoch 4/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9994 - loss: 0.0052 - val_accuracy: 0.9999 - val_loss: 9.8683e-04
Epoch 5/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9995 - loss: 0.0026 - val_accuracy: 0.9999 - val_loss: 5.2772e-04
Epoch 6/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9996 - loss: 0.0015 - val_accuracy: 0.9999 - val_loss: 3.2502e-04
Epoch 7/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9997 - val_loss: 0.0027
Epoch 8/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9997 - loss: 0.0012 - val_accuracy: 0.9997 - val_loss: 5.6232e-04
Epoch 9/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9997 - loss: 8.6971e-04 - val_accuracy: 0.9999 - val_loss: 2.2646e-04
Epoch 10/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 7.3303e-04 - val_accuracy: 0.9999 - val_loss: 3.9833e-04
Epoch 11/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 7.0285e-04 - val_accuracy: 1.0000 - val_loss: 6.4099e-06
Epoch 12/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 0.1805 - val_accuracy: 0.9999 - val_loss: 0.0024
Epoch 13/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 0.0013 - val_accuracy: 0.9999 - val_loss: 1.3108e-04
Epoch 14/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 0.0023 - val_accuracy: 0.9999 - val_loss: 4.1640e-04
Epoch 15/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 9.0971e-04 - val_accuracy: 0.9999 - val_loss: 1.3806e-04
Epoch 16/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 5.3028e-04 - val_accuracy: 1.0000 - val_loss: 8.0851e-06
[1m   1/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3:39[0m 140ms/step[1m  35/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step    [1m  71/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step[1m 111/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step[1m 146/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step[1m 181/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 217/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 254/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 291/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 329/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 365/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 402/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 436/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 469/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 505/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 538/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 573/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 607/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 637/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 669/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 700/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 731/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 759/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 789/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 819/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 854/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 889/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 926/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m 963/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m 998/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1035/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1069/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1103/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1137/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1170/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1202/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1236/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1259/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 1ms/step[1m1284/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1317/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1353/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1386/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1420/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1455/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1490/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 1ms/step[1m1524/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 1ms/step[1m1556/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 1ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 2ms/step
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Number of misclassified samples: 3
Misclassified samples per phase:
Phase 0: 3 samples
Phase 1: 0 samples
Phase 2: 0 samples
Phase 3: 0 samples
Phase 4: 0 samples
Phase 5: 0 samples

Misclassified samples plot saved to: experiments/results/lstm/sample_weights/w5/phase_1/misclassified_samples.png
=== LSTM Results ===
Accuracy:  0.9999
Precision: 0.7692
Recall:    1.0000
F1-Score:  0.8696
Confusion Matrix:
[[50320     3]
 [    0    10]]

END: 20251220T103826Z
RETURN_CODE: 0
