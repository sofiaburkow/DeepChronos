COMMAND: uv run python experiments/models/lstm.py experiments/preprocessing/processed_data/temporal/unbalanced/w10/phase_4/ True
START: 20251220T105031Z

2025-12-20 11:50:32.938752: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:50:33.016571: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-20 11:50:34.545166: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:50:35.907414: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Epoch 1/50
1062/1062 - 9s - 8ms/step - accuracy: 0.9897 - loss: 1.4234 - val_accuracy: 0.9919 - val_loss: 0.6622
Epoch 2/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9366 - loss: 0.7758 - val_accuracy: 0.9964 - val_loss: 0.7611
Epoch 3/50
1062/1062 - 7s - 6ms/step - accuracy: 0.8009 - loss: 0.4752 - val_accuracy: 0.8907 - val_loss: 0.3956
Epoch 4/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9293 - loss: 0.2277 - val_accuracy: 0.9572 - val_loss: 0.2383
Epoch 5/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9572 - loss: 0.2100 - val_accuracy: 0.9767 - val_loss: 0.2183
Epoch 6/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9667 - loss: 0.1334 - val_accuracy: 0.9603 - val_loss: 0.1008
Epoch 7/50
1062/1062 - 7s - 7ms/step - accuracy: 0.9771 - loss: 0.0664 - val_accuracy: 0.9944 - val_loss: 0.0549
Epoch 8/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9894 - loss: 0.0285 - val_accuracy: 0.9954 - val_loss: 0.0331
Epoch 9/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9936 - loss: 0.0193 - val_accuracy: 0.9959 - val_loss: 0.0233
Epoch 10/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9951 - loss: 0.0115 - val_accuracy: 0.9971 - val_loss: 0.0122
Epoch 11/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9960 - loss: 0.0107 - val_accuracy: 0.9974 - val_loss: 0.0067
Epoch 12/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9966 - loss: 0.0090 - val_accuracy: 0.9988 - val_loss: 0.0155
Epoch 13/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9980 - loss: 0.0066 - val_accuracy: 0.9985 - val_loss: 0.0052
Epoch 14/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9985 - loss: 0.0047 - val_accuracy: 0.9987 - val_loss: 0.0060
Epoch 15/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9985 - loss: 0.0038 - val_accuracy: 0.9991 - val_loss: 0.0051
Epoch 16/50
1062/1062 - 7s - 7ms/step - accuracy: 0.9989 - loss: 0.0260 - val_accuracy: 0.9911 - val_loss: 0.0188
Epoch 17/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9960 - loss: 0.0648 - val_accuracy: 0.9988 - val_loss: 0.0037
Epoch 18/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9982 - loss: 0.0061 - val_accuracy: 0.9988 - val_loss: 0.0040
Epoch 19/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9981 - loss: 0.0066 - val_accuracy: 0.9991 - val_loss: 0.0041
Epoch 20/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9981 - loss: 0.0135 - val_accuracy: 0.9985 - val_loss: 0.0060
Epoch 21/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9984 - loss: 0.0101 - val_accuracy: 0.9983 - val_loss: 0.0066
Epoch 22/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9981 - loss: 0.0051 - val_accuracy: 0.9988 - val_loss: 0.0036
Epoch 23/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9985 - loss: 0.0079 - val_accuracy: 0.9984 - val_loss: 0.0067
Epoch 24/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9984 - loss: 0.0050 - val_accuracy: 0.9989 - val_loss: 0.0039
Epoch 25/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9993 - loss: 0.0018 - val_accuracy: 0.9996 - val_loss: 0.6537
Epoch 26/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9987 - loss: 0.0816 - val_accuracy: 0.9988 - val_loss: 0.0055
Epoch 27/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9985 - loss: 0.0595 - val_accuracy: 0.9983 - val_loss: 0.0077
[1m   1/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m4:00[0m 153ms/step[1m  22/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 3ms/step    [1m  46/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m  69/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m  94/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 116/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 139/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 162/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 186/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 207/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 229/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 254/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 276/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 300/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 324/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 346/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 370/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 396/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 422/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 448/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 473/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 499/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 525/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 549/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 574/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 597/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 621/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 644/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 670/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 695/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 720/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 744/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 771/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 797/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 821/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 846/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 873/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 898/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 925/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 950/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 974/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 994/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1015/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1041/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1063/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1084/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1108/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1133/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1158/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1182/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1203/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1228/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1254/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1280/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1303/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1327/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1347/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1371/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1396/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1421/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1443/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1467/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1493/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1517/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1540/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1564/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 2ms/step
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Number of misclassified samples: 47
Misclassified samples per phase:
Phase 0: 39 samples
Phase 1: 0 samples
Phase 2: 1 samples
Phase 3: 6 samples
Phase 4: 0 samples
Phase 5: 1 samples

Misclassified samples plot saved to: experiments/results/lstm/sample_weights/w10/phase_4/misclassified_samples.png
=== LSTM Results ===
Accuracy:  0.9991
Precision: 0.1455
Recall:    1.0000
F1-Score:  0.2540
Confusion Matrix:
[[50281    47]
 [    0     8]]

END: 20251220T105342Z
RETURN_CODE: 0
