COMMAND: uv run python experiments/models/lstm.py experiments/preprocessing/processed_data/temporal/unbalanced/w10/phase_3/ False
START: 20251220T103013Z

2025-12-20 11:30:14.413839: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:30:14.479113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-20 11:30:15.928755: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:30:17.391548: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Epoch 1/50
1062/1062 - 8s - 7ms/step - accuracy: 0.9991 - loss: 0.0080 - val_accuracy: 0.9997 - val_loss: 0.0022
Epoch 2/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9999 - val_loss: 0.0011
Epoch 3/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9998 - loss: 0.0012 - val_accuracy: 0.9999 - val_loss: 7.2992e-04
Epoch 4/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9998 - loss: 7.7988e-04 - val_accuracy: 0.9999 - val_loss: 5.0279e-04
Epoch 5/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9998 - loss: 5.1492e-04 - val_accuracy: 0.9999 - val_loss: 3.5166e-04
Epoch 6/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9999 - loss: 4.1024e-04 - val_accuracy: 0.9999 - val_loss: 3.5800e-04
Epoch 7/50
1062/1062 - 7s - 6ms/step - accuracy: 0.9999 - loss: 2.9567e-04 - val_accuracy: 0.9999 - val_loss: 1.4191e-04
Epoch 8/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9999 - loss: 2.8027e-04 - val_accuracy: 0.9999 - val_loss: 4.3245e-04
Epoch 9/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9999 - loss: 2.0519e-04 - val_accuracy: 0.9999 - val_loss: 6.1298e-04
Epoch 10/50
1062/1062 - 6s - 6ms/step - accuracy: 1.0000 - loss: 1.7816e-04 - val_accuracy: 0.9999 - val_loss: 4.8596e-04
Epoch 11/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9999 - loss: 1.9576e-04 - val_accuracy: 0.9999 - val_loss: 4.5732e-04
Epoch 12/50
1062/1062 - 6s - 6ms/step - accuracy: 0.9999 - loss: 1.8030e-04 - val_accuracy: 0.9999 - val_loss: 4.4063e-04
[1m   1/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3:44[0m 143ms/step[1m  25/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step    [1m  51/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m  76/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 100/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 124/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 2ms/step[1m 147/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 171/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 194/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 219/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 244/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 268/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 292/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 318/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 344/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 370/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 394/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 419/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 442/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 466/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 491/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 517/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 544/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 571/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 596/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 620/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 645/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 670/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 694/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 720/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 747/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 773/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 798/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 824/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 851/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 875/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 900/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 913/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 933/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 957/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m 982/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1005/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1030/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1054/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1073/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1096/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m1s[0m 2ms/step[1m1120/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1145/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1170/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1196/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1221/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1246/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 2ms/step[1m1269/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1291/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1316/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 2ms/step[1m1341/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1366/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1391/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 2ms/step[1m1416/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1440/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1463/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1487/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 2ms/step[1m1510/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1535/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1561/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 2ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 2ms/step
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Number of misclassified samples: 9
Misclassified samples per phase:
Phase 0: 3 samples
Phase 1: 0 samples
Phase 2: 1 samples
Phase 3: 5 samples
Phase 4: 0 samples
Phase 5: 0 samples

Misclassified samples plot saved to: experiments/results/lstm/no_sample_weights/w10/phase_3/misclassified_samples.png
=== LSTM Results ===
Accuracy:  0.9998
Precision: 0.7143
Recall:    0.6667
F1-Score:  0.6897
Confusion Matrix:
[[50317     4]
 [    5    10]]

END: 20251220T103142Z
RETURN_CODE: 0
