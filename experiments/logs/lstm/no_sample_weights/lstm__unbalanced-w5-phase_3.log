COMMAND: uv run python experiments/models/lstm.py experiments/preprocessing/processed_data/temporal/unbalanced/w5/phase_3/ False
START: 20251220T102529Z

2025-12-20 11:25:30.302809: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:25:30.367675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-12-20 11:25:31.821803: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-20 11:25:33.190704: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
/home/sofia/Desktop/Thesis/DeepChronos/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(**kwargs)
Epoch 1/50
1062/1062 - 6s - 5ms/step - accuracy: 0.9979 - loss: 0.0117 - val_accuracy: 0.9997 - val_loss: 0.0016
Epoch 2/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9997 - loss: 0.0016 - val_accuracy: 0.9997 - val_loss: 0.0011
Epoch 3/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 9.9387e-04 - val_accuracy: 0.9999 - val_loss: 6.8217e-04
Epoch 4/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 6.0772e-04 - val_accuracy: 0.9999 - val_loss: 3.8214e-04
Epoch 5/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9998 - loss: 4.5870e-04 - val_accuracy: 0.9999 - val_loss: 2.0993e-04
Epoch 6/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 3.0470e-04 - val_accuracy: 0.9999 - val_loss: 1.7395e-04
Epoch 7/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 3.3681e-04 - val_accuracy: 0.9999 - val_loss: 1.4844e-04
Epoch 8/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 3.0073e-04 - val_accuracy: 0.9999 - val_loss: 4.1601e-04
Epoch 9/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 2.5514e-04 - val_accuracy: 0.9999 - val_loss: 2.6456e-04
Epoch 10/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 2.2533e-04 - val_accuracy: 0.9999 - val_loss: 2.3538e-04
Epoch 11/50
1062/1062 - 4s - 4ms/step - accuracy: 0.9999 - loss: 2.0660e-04 - val_accuracy: 0.9999 - val_loss: 1.6934e-04
Epoch 12/50
1062/1062 - 4s - 4ms/step - accuracy: 1.0000 - loss: 1.2384e-04 - val_accuracy: 0.9999 - val_loss: 9.1064e-04
[1m   1/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3:28[0m 132ms/step[1m  34/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step    [1m  68/1573[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 101/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 2ms/step[1m 137/1573[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step[1m 174/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 1ms/step[1m 211/1573[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 248/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 281/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 314/1573[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 349/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 386/1573[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 421/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 457/1573[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 492/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 525/1573[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 559/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 595/1573[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 631/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 666/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 701/1573[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 738/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 775/1573[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 810/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 844/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 879/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m1s[0m 1ms/step[1m 915/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m 950/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m 986/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1024/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1061/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1098/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1134/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1171/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1208/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1244/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 1ms/step[1m1281/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 1ms/step[1m1319/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 1ms/step[1m1356/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 1ms/step[1m1391/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 1ms/step[1m1429/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 1ms/step[1m1465/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 1ms/step[1m1500/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 1ms/step[1m1535/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 1ms/step[1m1570/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 1ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 1ms/step[1m1573/1573[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 2ms/step
WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. 
Number of misclassified samples: 4
Misclassified samples per phase:
Phase 0: 1 samples
Phase 1: 0 samples
Phase 2: 0 samples
Phase 3: 3 samples
Phase 4: 0 samples
Phase 5: 0 samples

Misclassified samples plot saved to: experiments/results/lstm/no_sample_weights/w5/phase_3/misclassified_samples.png
=== LSTM Results ===
Accuracy:  0.9999
Precision: 0.9167
Recall:    0.7857
F1-Score:  0.8462
Confusion Matrix:
[[50318     1]
 [    3    11]]

END: 20251220T102627Z
RETURN_CODE: 0
